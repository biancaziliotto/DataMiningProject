{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/final_dataset.csv\")\n",
    "print(\"Shape of dataset:\", dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are NaN values:\", dataset.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_whiskers(df, column):\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_whisker = q1 - 1.5 * iqr\n",
    "    upper_whisker = q3 + 1.5 * iqr\n",
    "    return lower_whisker, upper_whisker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = dataset.boxplot(column=['povertyPercentage'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_whisker_poverty, upper_whisker_poverty = return_whiskers(dataset, 'povertyPercentage')\n",
    "print(\"Amount of outliers in povertyPercentage:\", dataset[(dataset['povertyPercentage'] < lower_whisker_poverty) | (dataset['povertyPercentage'] > upper_whisker_poverty)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for poverty percentages that are outliers are still realistic and close to whiskers. Furthermore, poverty percentage is a key attribute of our analysis, so we decide to keep all the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = dataset.boxplot(column = ['avg_age_participants'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = dataset.boxplot(column = ['killed_ratio'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop outliers for unrealistic values of avg age participants which may negatively influence clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get whisker value of boxplot for avg_age_participants\n",
    "lower_whisker_avg_age, upper_whisker_avg_age = return_whiskers(dataset, 'avg_age_participants')\n",
    "\n",
    "# Drop rows with avg_age_participants > upper_whisker\n",
    "dataset = dataset[dataset['avg_age_participants'] <= upper_whisker_avg_age]\n",
    "print(\"Shape of dataset after removing outliers:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = dataset.boxplot(column = ['democrats_ratio', 'republicans_ratio'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of 100% or 0% are pretty unrealistic, so we decide to drop all over 95% (or below 5%), considering them as outliers / wrongly observed data. Moreover, we will keep only one of the two ratios, and also drop the winning party attribute. For this reason, we decide to drop strange observations where the party that won had less than 50% of the votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of reublicans_ratio outliers:\", dataset[dataset['republicans_ratio'] == 1].shape[0] + dataset[dataset['republicans_ratio'] == 0].shape[0])\n",
    "print(\"Amount of democrats_ratio outliers:\", dataset[dataset['democrats_ratio'] == 1].shape[0] + dataset[dataset['democrats_ratio'] == 0].shape[0])\n",
    "\n",
    "# Drop rows with republicans_ratio <= 0.05 or >= 0.95\n",
    "dataset = dataset[(dataset['republicans_ratio'] > 0.05) & (dataset['republicans_ratio'] < 0.95)]\n",
    "print(\"Shape of dataset after removing outliers:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many times a party won and the percentage of votes for that part was <0.5\n",
    "democrats_strange = len(dataset[(dataset['republicans_ratio'] > 0.5 ) & (dataset['party'] == 0)])\n",
    "republicans_strange = len(dataset[(dataset['republicans_ratio'] < 0.5 ) & (dataset['party'] == 1)])\n",
    "\n",
    "print(\"Amount of times democrats won and the percentage of votes for that part was <0.5:\", democrats_strange)\n",
    "print(\"Amount of times republicans won and the percentage of votes for that part was <0.5:\", republicans_strange)\n",
    "print(\"Amount of strange wins:\", democrats_strange + republicans_strange)\n",
    "\n",
    "# Drop rows with republican strange wins\n",
    "dataset = dataset[((dataset['republicans_ratio'] > 0.5) & (dataset['party'] == 1)) | ((dataset['republicans_ratio'] < 0.5) & (dataset['party'] == 0))]\n",
    "print(\"Shape of dataset after removing outliers:\", dataset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dropping columns for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = ['min_age_participants', 'max_age_participants', 'teen_ratio', 'totalvotes', 'year', 'party', 'democrats_ratio']\n",
    "print(\"Attributes to drop:\", dropped_columns)\n",
    "dataset_reduced = dataset.drop(columns=dropped_columns, axis = 1)\n",
    "print(\"Shape of dataset:\", dataset_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_state = \"Florida\"\n",
    "dataset_reduced_florida = dataset_reduced[dataset_reduced[\"state_\" + selected_state] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = [c for c in dataset_reduced.columns if c.startswith('state_')]\n",
    "print(\"Attributes to drop:\", dropped_columns)\n",
    "\n",
    "dataset_reduced = dataset_reduced.drop(columns=dropped_columns, axis = 1)\n",
    "dataset_reduced_florida = dataset_reduced_florida.drop(columns=dropped_columns, axis = 1)\n",
    "\n",
    "print(\"Shape of dataset:\", dataset_reduced.shape)\n",
    "print(\"Shape of dataset for florida:\", dataset_reduced_florida.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_dataset = scaler.fit_transform(dataset_reduced.values)\n",
    "\n",
    "scaler_florida= MinMaxScaler()\n",
    "scaled_dataset_florida = scaler_florida.fit_transform(dataset_reduced_florida.values)\n",
    "\n",
    "print(\"Shape of scaled dataset:\", scaled_dataset.shape)\n",
    "print(\"Shape of scaled dataset for florida:\", scaled_dataset_florida.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Identification of the best value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_list = []\n",
    "silhouette_list = []\n",
    "davies_bouldin_list = []\n",
    "\n",
    "max_k = 20\n",
    "for k in tqdm(range(2, max_k + 1), ):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(scaled_dataset)\n",
    "\n",
    "    sse_list.append(kmeans.inertia_)\n",
    "    silhouette_list.append(silhouette_score(scaled_dataset, kmeans.labels_, sample_size=10000))\n",
    "    davies_bouldin_list.append(davies_bouldin_score(scaled_dataset, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list) + 2), sse_list)\n",
    "ax[0].set_ylabel('SSE', fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list) + 2), silhouette_list)\n",
    "ax[1].set_ylabel('Silhouette Score', fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list) + 2))\n",
    "\n",
    "ax[2].plot(range(2, len(davies_bouldin_list) + 2), davies_bouldin_list)\n",
    "ax[2].set_ylabel('Davies Bouldin Score', fontsize=22)\n",
    "ax[2].set_xticks(range(2, len(davies_bouldin_list) + 2))\n",
    "\n",
    "\n",
    "plt.xlabel('K', fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "# NICER PLOTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analysis of the centroids and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 9\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "kmeans.fit(scaled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of scaled_dataset: \", scaled_dataset.shape)\n",
    "print(\"Shape of kmeans.labels_: \", kmeans.labels_.shape)\n",
    "print(\"Shape of kmeans.cluster_centers_: \", kmeans.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "centers_df = pd.DataFrame(centers, columns=dataset_reduced.columns)\n",
    "centers_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num_points = []\n",
    "cluster_points = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_points.append(scaled_dataset[kmeans.labels_ == i])\n",
    "    cluster_num_points.append(len(cluster_points[-1]))\n",
    "    \n",
    "cluster_num_points, cluster_points[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_tot = KMeans(n_clusters=1, n_init=10)\n",
    "kmeans_tot.fit(scaled_dataset)\n",
    "total_SSE = kmeans_tot.inertia_ / len(scaled_dataset)\n",
    "\n",
    "cluster_SSE = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_sse = 0\n",
    "    for point in cluster_points[i]:\n",
    "        cluster_sse += np.linalg.norm(point - kmeans.cluster_centers_[i])**2\n",
    "    cluster_SSE.append(cluster_sse / cluster_num_points[i])\n",
    "\n",
    "print(\"Cluster SSE: \", cluster_SSE)\n",
    "print(\"Total SSE: \", total_SSE)\n",
    "print(\"SSE of cluster with min SSE: \", min(cluster_SSE))\n",
    "print(\"SSE of cluster with max SSE: \", max(cluster_SSE))\n",
    "print(\"Mean of SSE: \", np.mean(cluster_SSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_distance_variance = []\n",
    "total_distance_variance = 0\n",
    "dataset_centroid = np.mean(scaled_dataset, axis=0)\n",
    "\n",
    "###### CI RIPENSIAMO SU QUESTO ######\n",
    "for point in scaled_dataset:\n",
    "    total_distance_variance += (np.linalg.norm(point - dataset_centroid)**2 - total_SSE) ** 2\n",
    "total_distance_variance /= len(scaled_dataset)\n",
    "#####################################\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    variance = 0\n",
    "    for p in cluster_points[i]:\n",
    "        variance += ((np.linalg.norm(p - kmeans.cluster_centers_[i]))**2 - cluster_SSE[i])**2\n",
    "    cluster_distance_variance.append(variance/cluster_num_points[i])\n",
    "\n",
    "print(\"Cluster distance variance: \", cluster_distance_variance)\n",
    "print(\"Total distance variance: \", total_distance_variance)\n",
    "print(\"Distance variance of cluster with min SSE: \", min(cluster_distance_variance))\n",
    "print(\"Distance variance of cluster with max SSE: \", max(cluster_distance_variance))\n",
    "print(\"Mean of distance variance: \", np.mean(cluster_distance_variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a distance matrix among cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_distance_matrix = squareform(pdist(kmeans.cluster_centers_))\n",
    "sns.heatmap(centroid_distance_matrix, annot=True, fmt = '.2f', cmap='crest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct matrix displaying correlation of attribute values to belonging to a certain cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom kmeans labels into onehot encoding\n",
    "onehot = np.zeros((len(kmeans.labels_), n_clusters))\n",
    "onehot[np.arange(len(kmeans.labels_)), kmeans.labels_] = 1\n",
    "\n",
    "# Compute correlation between onehot encoding and scaled dataset\n",
    "onehot_corr = np.corrcoef(scaled_dataset, onehot, rowvar=False)\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(onehot_corr[:scaled_dataset.shape[1], scaled_dataset.shape[1]:], cmap=cmap)\n",
    "\n",
    "# Set ticks on y axis with feature names\n",
    "plt.yticks(np.arange(scaled_dataset.shape[1]) + 0.5, dataset_reduced.columns, rotation=0, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity matrix of a sample of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the dataset to 10000\n",
    "samples = np.random.choice(scaled_dataset.shape[0], 1000, replace=False)\n",
    "downsampled_dataset = scaled_dataset[samples]\n",
    "downsampled_labels = kmeans.labels_[samples]\n",
    "\n",
    "# Sort based on labels\n",
    "sorted_indexes = np.argsort(downsampled_labels)\n",
    "downsampled_dataset = downsampled_dataset[sorted_indexes]\n",
    "downsampled_labels = downsampled_labels[sorted_indexes]\n",
    "\n",
    "# Compute similarity matrix\n",
    "pdist_matrix = squareform(pdist(downsampled_dataset, metric='minkowski', p=2))\n",
    "sns.heatmap(pdist_matrix, fmt = '.2f', cmap='crest')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, n_jobs=-1)\n",
    "tsne_dataset = np.concatenate((downsampled_dataset, kmeans.cluster_centers_))\n",
    "tsne_labels = np.concatenate((downsampled_labels, range(n_clusters)))\n",
    "tsne_map = tsne.fit_transform(tsne_dataset)\n",
    "\n",
    "scatter = plt.scatter(tsne_map[:-n_clusters, 0], tsne_map[:-n_clusters, 1], c = tsne_labels[:-n_clusters], s=10, cmap='tab10')\n",
    "plt.scatter(tsne_map[-n_clusters:, 0], tsne_map[-n_clusters:, 1], c = tsne_labels[-n_clusters:], s=100, cmap='tab10', marker='*', edgecolors='black')\n",
    "\n",
    "#for i in range(n_clusters):\n",
    "#    plt.annotate(i, tsne_map[-n_clusters + i, :], fontsize=20)\n",
    "\n",
    "# Get unique cluster labels\n",
    "unique_labels = set(tsne_labels[:-n_clusters])\n",
    "\n",
    "# Create a legend with a color for each cluster\n",
    "legend_entries = []\n",
    "for label in unique_labels:\n",
    "    # Find the indices of data points with the current label\n",
    "    indices = tsne_labels[:-n_clusters] == label\n",
    "    # Add a legend entry for the current label with the corresponding color\n",
    "    legend_entries.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=scatter.cmap(scatter.norm(label)), markersize=8, label=f'Cluster {label}'))\n",
    "\n",
    "# Add legend to the plot\n",
    "plt.legend(handles=legend_entries, loc='lower right')\n",
    "plt.xlim(-50,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Distribution of variables: within clusters vs whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of distribution of repubblican vs democrats in the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_xt_pct = pd.crosstab(kmeans.labels_, dataset_reduced['republicans_ratio'] > 0.5)\n",
    "party_xt_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_xt_pct.plot(kind='bar', stacked=False, \n",
    "                   title='Party per cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Party')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "populous_city_xt_pct = pd.crosstab(kmeans.labels_, dataset_reduced['populous_city'])\n",
    "populous_city_xt_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populous_city_xt_pct.plot(kind='bar', stacked=False, \n",
    "                   title='Popolous city per cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Party')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some continuous variables, such as the arrested ratio, and look at clusters positively and negatively correlated to that variable. We can see that the distributions are very different amongst the two clusters, and they represent different trends which are present in the full distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of average age for whole dataset and clusters\n",
    "arrested_analysis = [3, 4]\n",
    "dataset_reduced_with_clusters = dataset_reduced.copy()\n",
    "dataset_reduced_with_clusters['cluster'] = kmeans.labels_\n",
    "dataset_reduced_with_clusters_3_4 = dataset_reduced_with_clusters[(dataset_reduced_with_clusters['cluster'] == arrested_analysis[0]) | \\\n",
    "                                                              (dataset_reduced_with_clusters['cluster'] == arrested_analysis[1]) ]  \n",
    "\n",
    "sns.displot(dataset_reduced_with_clusters_3_4, x=\"arrested_ratio\", kind='kde', hue=\"cluster\")\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "\n",
    "sns.displot(dataset_reduced_with_clusters, x=\"arrested_ratio\", kind='kde')\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Identification of best eps value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:43:59.704375610Z",
     "start_time": "2023-11-25T10:43:58.676260990Z"
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree', n_jobs=-1).fit(scaled_dataset_florida)\n",
    "distances, indices = nbrs.kneighbors(scaled_dataset_florida)\n",
    "kth_distances = distances[:, k]\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:00.384258462Z",
     "start_time": "2023-11-25T10:44:00.247368160Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(kth_distances)), sorted(kth_distances))\n",
    "plt.axhline(y = 0.5, color = 'r')\n",
    "plt.ylabel('dist from %sth neighbor' % k, fontsize=18)\n",
    "plt.xlabel('sorted distances', fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:17.830241301Z",
     "start_time": "2023-11-25T10:44:17.715215799Z"
    }
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.55, min_samples=150)\n",
    "dbscan.fit(scaled_dataset_florida)\n",
    "labels, cluster_num_points = np.unique(dbscan.labels_, return_counts=True)\n",
    "n_clusters = len(labels)\n",
    "cluster_num_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:19.475084303Z",
     "start_time": "2023-11-25T10:44:19.374596721Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_points = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_points.append(scaled_dataset_florida[dbscan.labels_ == i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:21.620248236Z",
     "start_time": "2023-11-25T10:44:20.289522329Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Silhouette %s' % silhouette_score(scaled_dataset_florida, dbscan.labels_))\n",
    "print('Davies-Bouldin %s' % davies_bouldin_score(scaled_dataset_florida, dbscan.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:22.394838891Z",
     "start_time": "2023-11-25T10:44:22.106855355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transfrom kmeans labels into onehot encoding\n",
    "onehot = np.zeros((len(dbscan.labels_), n_clusters))\n",
    "onehot[np.arange(len(dbscan.labels_)), dbscan.labels_] = 1\n",
    "\n",
    "# Compute correlation between onehot encoding and scaled dataset\n",
    "onehot_corr = np.corrcoef(scaled_dataset_florida, onehot, rowvar=False)\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(onehot_corr[:scaled_dataset_florida.shape[1], scaled_dataset_florida.shape[1]:], fmt = '.2f', cmap=cmap)\n",
    "\n",
    "# Set ticks on y axis with feature names\n",
    "plt.yticks(np.arange(scaled_dataset_florida.shape[1]) + 0.5, dataset_reduced_florida.columns, rotation=0, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:24.219052411Z",
     "start_time": "2023-11-25T10:44:22.690480770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Downsample the dataset to 10000\n",
    "samples = np.random.choice(scaled_dataset_florida.shape[0], 1000, replace=False)\n",
    "downsampled_dataset = scaled_dataset_florida[samples]\n",
    "downsampled_labels = dbscan.labels_[samples]\n",
    "\n",
    "# Sort based on labels\n",
    "sorted_indexes = np.argsort(downsampled_labels)\n",
    "downsampled_dataset = downsampled_dataset[sorted_indexes]\n",
    "downsampled_labels = downsampled_labels[sorted_indexes]\n",
    "\n",
    "# Compute similarity matrix\n",
    "pdist_matrix = squareform(pdist(downsampled_dataset, metric='minkowski', p=2))\n",
    "sns.heatmap(pdist_matrix, fmt = '.2f', cmap='crest')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:48.542153691Z",
     "start_time": "2023-11-25T10:44:24.212454958Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, n_jobs=-1)\n",
    "tsne_dataset = tsne.fit_transform(scaled_dataset_florida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:48.677916259Z",
     "start_time": "2023-11-25T10:44:48.542042326Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = ['gray', 'red', 'blue', 'green', 'yellow', 'purple', 'orange', 'cyan', 'brown']\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(tsne_dataset[:,0][dbscan.labels_ == i-1], tsne_dataset[:,1][dbscan.labels_ == i-1], s=3, c=colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:54.598733203Z",
     "start_time": "2023-11-25T10:44:48.677455931Z"
    }
   },
   "outputs": [],
   "source": [
    "umap_reducer = UMAP().fit(scaled_dataset_florida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:54.761533146Z",
     "start_time": "2023-11-25T10:44:54.601548193Z"
    }
   },
   "outputs": [],
   "source": [
    "umap_dataset = umap_reducer.transform(scaled_dataset)\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(umap_dataset[:,0][dbscan.labels_ == i-1], umap_dataset[:,1][dbscan.labels_ == i-1], s=3, c=colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:44:55.486550181Z",
     "start_time": "2023-11-25T10:44:55.310018053Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.scatter(dataset_reduced_florida[\"longitude\"][dbscan.labels_ == i-1], dataset_reduced_florida[\"latitude\"][dbscan.labels_ == i-1], s=10)\n",
    "\n",
    "plt.xlim(-88, -78)\n",
    "plt.ylim(26, 36)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Distribution of variables: within clusters vs whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:06:28.463854230Z",
     "start_time": "2023-11-25T08:06:28.415629493Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:03:01.949339184Z",
     "start_time": "2023-11-25T08:03:00.236656231Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_state = \"Florida\"\n",
    "dataset = pd.read_csv(\"data/final_dataset.csv\")\n",
    "dataset = dataset[dataset[\"state_\" + selected_state] == True]\n",
    "\n",
    "dropped_columns = [c for c in dataset.columns if c.startswith('state_')]\n",
    "dataset_reduced = dataset.drop(columns=dropped_columns, axis = 1)\n",
    "dataset_reduced.drop(columns=[\"min_age_participants\", \"max_age_participants\", \"totalvotes\", \"teen_ratio\", \"povertyPercentage\", \"year\", \"democrats_ratio\", \"party\"], axis=1, inplace=True)\n",
    "\n",
    "dataset_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:03:03.038557656Z",
     "start_time": "2023-11-25T08:03:03.024433921Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_dataset = dataset_reduced._get_numeric_data()\n",
    "numeric_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:03:07.843224198Z",
     "start_time": "2023-11-25T08:03:07.837150432Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_dataset.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:03:09.593584492Z",
     "start_time": "2023-11-25T08:03:09.569122084Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_dataset = scaler.fit_transform(numeric_dataset.values)\n",
    "scaled_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:32:42.630409704Z",
     "start_time": "2023-11-25T08:32:42.602286565Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "    \n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:30:58.359321258Z",
     "start_time": "2023-11-25T08:30:48.334687264Z"
    }
   },
   "outputs": [],
   "source": [
    "linkages = [\"single\", \"complete\", \"average\", \"ward\"]\n",
    "hierarchical_results = []\n",
    "for linkage in linkages:\n",
    "    hierarchical_clustering = AgglomerativeClustering(linkage=linkage, distance_threshold=0, n_clusters=None)\n",
    "    hierarchical_clustering.fit(scaled_dataset)\n",
    "    hierarchical_results.append(hierarchical_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T08:33:24.879000862Z",
     "start_time": "2023-11-25T08:33:23.349788123Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "for i, (linkage, model) in enumerate(zip(linkages, hierarchical_results)):\n",
    "    # plot the top three levels of the dendrogram\n",
    "    ax[i // 2][i % 2].set_title(linkage)\n",
    "    plot_dendrogram(model, truncate_mode=\"level\", p=3, ax=ax[i // 2][i % 2], orientation=\"right\")\n",
    "    ax[i // 2][i % 2].set_ylabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:13:29.782531243Z",
     "start_time": "2023-11-25T09:13:29.730827209Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_sse(labels):\n",
    "    n_cluster = len(np.unique(labels))\n",
    "    global_sse = 0\n",
    "    for i in range(n_cluster): # for each cluster\n",
    "        cluster_points = scaled_dataset[labels == i]\n",
    "        centroid = np.mean(cluster_points, axis=0)\n",
    "        # calculate the sse for the single cluster\n",
    "        current_sse = 0\n",
    "        for p in cluster_points:\n",
    "            current_sse += np.linalg.norm( np.subtract(p, centroid)) ** 2\n",
    "\n",
    "        global_sse += current_sse\n",
    "    return global_sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:14:36.202706019Z",
     "start_time": "2023-11-25T09:13:30.425699942Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sse_list = []\n",
    "silhoutte_list = []\n",
    "davies_bouldin_list = []\n",
    "\n",
    "max_k = 20\n",
    "for k in tqdm(range(2, max_k + 1), ):\n",
    "    ward_clustering = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
    "    ward_clustering.fit(scaled_dataset)\n",
    "\n",
    "    sse_list.append(calculate_sse(ward_clustering.labels_))\n",
    "    silhoutte_list.append(silhouette_score(scaled_dataset, ward_clustering.labels_, sample_size=10000))\n",
    "    davies_bouldin_list.append(davies_bouldin_score(scaled_dataset, ward_clustering.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T09:14:54.265768684Z",
     "start_time": "2023-11-25T09:14:53.638084935Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list) + 2), sse_list)\n",
    "ax[0].set_ylabel('SSE', fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhoutte_list) + 2), silhoutte_list)\n",
    "ax[1].set_ylabel('Silhouette Score', fontsize=22)\n",
    "\n",
    "ax[2].plot(range(2, len(davies_bouldin_list) + 2), davies_bouldin_list)\n",
    "ax[2].set_xlabel('K', fontsize=22)\n",
    "ax[2].set_ylabel('Davies Bouldin Score', fontsize=22)\n",
    "\n",
    "\n",
    "plt.xlabel('K', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:31:49.239836644Z",
     "start_time": "2023-11-25T10:31:46.885624830Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = 8\n",
    "ward_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "ward_clustering.fit(scaled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:31:49.386508383Z",
     "start_time": "2023-11-25T10:31:49.236327713Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_points = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_points.append(scaled_dataset[ward_clustering.labels_ == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:31:50.295780684Z",
     "start_time": "2023-11-25T10:31:49.240687060Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Silhouette %s' % silhouette_score(scaled_dataset, ward_clustering.labels_))\n",
    "print('Davies-Bouldin %s' % davies_bouldin_score(scaled_dataset, ward_clustering.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:31:50.646642493Z",
     "start_time": "2023-11-25T10:31:50.296035843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transfrom kmeans labels into onehot encoding\n",
    "onehot = np.zeros((len(ward_clustering.labels_), n_clusters))\n",
    "onehot[np.arange(len(ward_clustering.labels_)), ward_clustering.labels_] = 1\n",
    "\n",
    "# Compute correlation between onehot encoding and scaled dataset\n",
    "onehot_corr = np.corrcoef(scaled_dataset, onehot, rowvar=False)\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(onehot_corr[:scaled_dataset.shape[1], scaled_dataset.shape[1]:], fmt = '.2f', cmap=cmap)\n",
    "\n",
    "# Set ticks on y axis with feature names\n",
    "plt.yticks(np.arange(scaled_dataset.shape[1]) + 0.5, numeric_dataset.columns, rotation=0, fontsize=12)\n",
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:31:52.240699783Z",
     "start_time": "2023-11-25T10:31:50.649910818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Downsample the dataset to 10000\n",
    "samples = np.random.choice(scaled_dataset.shape[0], 1000, replace=False)\n",
    "downsampled_dataset = scaled_dataset[samples]\n",
    "downsampled_labels = ward_clustering.labels_[samples]\n",
    "\n",
    "# Sort based on labels\n",
    "sorted_indexes = np.argsort(downsampled_labels)\n",
    "downsampled_dataset = downsampled_dataset[sorted_indexes]\n",
    "downsampled_labels = downsampled_labels[sorted_indexes]\n",
    "\n",
    "# Compute similarity matrix\n",
    "pdist_matrix = squareform(pdist(downsampled_dataset, metric='minkowski', p=2))\n",
    "sns.heatmap(pdist_matrix, fmt = '.2f', cmap='crest')\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:32:22.901370116Z",
     "start_time": "2023-11-25T10:31:52.227468054Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, n_jobs=-1)\n",
    "tsne_dataset = tsne.fit_transform(scaled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:58:50.347141772Z",
     "start_time": "2023-11-25T10:58:50.106712457Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'cyan', 'brown']\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(tsne_dataset[:,0][ward_clustering.labels_ == i], tsne_dataset[:,1][ward_clustering.labels_ == i], s=3, c=colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:32:31.533813204Z",
     "start_time": "2023-11-25T10:32:23.135167718Z"
    }
   },
   "outputs": [],
   "source": [
    "umap_reducer = UMAP().fit(scaled_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:58:53.110171132Z",
     "start_time": "2023-11-25T10:58:52.937793043Z"
    }
   },
   "outputs": [],
   "source": [
    "umap_dataset = umap_reducer.transform(scaled_dataset)\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(umap_dataset[:,0][ward_clustering.labels_ == i], umap_dataset[:,1][ward_clustering.labels_ == i], s=3, c=colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:58:55.124676262Z",
     "start_time": "2023-11-25T10:58:54.952481899Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.scatter(numeric_dataset[\"males_ratio\"][ward_clustering.labels_ == i], numeric_dataset[\"avg_age_participants\"][ward_clustering.labels_ == i], c=colors[i], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:58:59.594295912Z",
     "start_time": "2023-11-25T10:58:59.574827194Z"
    }
   },
   "outputs": [],
   "source": [
    "party_xt_pct = pd.crosstab(ward_clustering.labels_, numeric_dataset['republicans_ratio'] > 0.5)\n",
    "party_xt_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:59:00.515464995Z",
     "start_time": "2023-11-25T10:59:00.388855116Z"
    }
   },
   "outputs": [],
   "source": [
    "party_xt_pct.plot(kind='bar', stacked=False, \n",
    "                   title='Party per cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Party')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:59:01.214310174Z",
     "start_time": "2023-11-25T10:59:01.065229639Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.scatter(numeric_dataset[\"adults_ratio\"][ward_clustering.labels_ == i-1], numeric_dataset[\"killed_ratio\"][ward_clustering.labels_ == i-1], s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:59:01.938490738Z",
     "start_time": "2023-11-25T10:59:01.730056465Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.scatter(numeric_dataset[\"longitude\"][ward_clustering.labels_ == i-1], numeric_dataset[\"latitude\"][ward_clustering.labels_ == i-1], s=10)\n",
    "\n",
    "plt.xlim(-88, -78)\n",
    "plt.ylim(26, 36)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-25T08:03:27.988990215Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-25T08:03:27.989042079Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
