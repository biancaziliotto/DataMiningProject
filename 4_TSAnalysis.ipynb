{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shelve\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Notebook Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_file = 'vars.shelve'\n",
    "\n",
    "LOAD_VARS = True\n",
    "\n",
    "if LOAD_VARS and os.path.exists(vars_file):\n",
    "    with shelve.open(vars_file) as db:\n",
    "        for key in db:\n",
    "            globals()[key] = db[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (239381, 76)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>city_or_county</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>min_age_participants</th>\n",
       "      <th>avg_age_participants</th>\n",
       "      <th>max_age_participants</th>\n",
       "      <th>teen_ratio</th>\n",
       "      <th>adults_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>state_Texas</th>\n",
       "      <th>state_Utah</th>\n",
       "      <th>state_Vermont</th>\n",
       "      <th>state_Virginia</th>\n",
       "      <th>state_Washington</th>\n",
       "      <th>state_West Virginia</th>\n",
       "      <th>state_Wisconsin</th>\n",
       "      <th>state_Wyoming</th>\n",
       "      <th>month_x</th>\n",
       "      <th>month_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>39.8322</td>\n",
       "      <td>-86.2492</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Kane</td>\n",
       "      <td>41.6645</td>\n",
       "      <td>-78.7856</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1404.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>42.4190</td>\n",
       "      <td>-83.0393</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1383.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>38.9030</td>\n",
       "      <td>-76.9820</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>894.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>40.4621</td>\n",
       "      <td>-80.0308</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date    year city_or_county  latitude  longitude  min_age_participants  \\\n",
       "0   851.0  2015.0   Indianapolis   39.8322   -86.2492                  19.0   \n",
       "1  1553.0  2017.0           Kane   41.6645   -78.7856                  62.0   \n",
       "2  1404.0  2016.0        Detroit   42.4190   -83.0393                  28.0   \n",
       "3  1383.0  2016.0     Washington   38.9030   -76.9820                  18.0   \n",
       "4   894.0  2015.0     Pittsburgh   40.4621   -80.0308                  28.0   \n",
       "\n",
       "   avg_age_participants  max_age_participants  teen_ratio  adults_ratio  ...  \\\n",
       "0                  19.0                  19.0         0.0           1.0  ...   \n",
       "1                  62.0                  62.0         0.0           1.0  ...   \n",
       "2                  28.0                  28.0         0.0           1.0  ...   \n",
       "3                  28.0                  37.0         0.0           1.0  ...   \n",
       "4                  28.0                  28.0         0.0           1.0  ...   \n",
       "\n",
       "   state_Texas  state_Utah  state_Vermont  state_Virginia  state_Washington  \\\n",
       "0        False       False          False           False             False   \n",
       "1        False       False          False           False             False   \n",
       "2        False       False          False           False             False   \n",
       "3        False       False          False           False             False   \n",
       "4        False       False          False           False             False   \n",
       "\n",
       "   state_West Virginia  state_Wisconsin  state_Wyoming       month_x   month_y  \n",
       "0                False            False          False  5.000000e-01 -0.866025  \n",
       "1                False            False          False  8.660254e-01 -0.500000  \n",
       "2                False            False          False -5.000000e-01  0.866025  \n",
       "3                False            False          False -8.660254e-01  0.500000  \n",
       "4                False            False          False  1.224647e-16 -1.000000  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"data/final_dataset.csv\")\n",
    "print(\"Shape of dataset:\", dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Select cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2017.0    61389\n",
       "2016.0    58724\n",
       "2015.0    53335\n",
       "2014.0    51684\n",
       "2018.0    13801\n",
       "2013.0      448\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only incidents regarding [2014, 2015, 2016, 2017], as by project assignment instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[(dataset['year'] > 2013) & (dataset['year'] < 2018)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of cities reveals that many cities are present with different names, resulting in incorrect city value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12596 unique cities in the dataset\n"
     ]
    }
   ],
   "source": [
    "with open('debugging/cities.txt', 'w') as f:\n",
    "    for item in dataset['city_or_county'].unique():\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# Write city and value counts of each city to a file\n",
    "with open('debugging/city_counts.txt', 'w') as f:\n",
    "    f.write(dataset['city_or_county'].value_counts().to_string())\n",
    "\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate parenthesis with county or extra information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11762 unique cities in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Eliminate all data between parenthesis in the city column using re module\n",
    "dataset['city_or_county'] = dataset['city_or_county'].apply(lambda x: re.sub(r\"\\(.*\\)\", \"\", x))\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort cities alphabetically to see if there are still duplicates and how relevant they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all cities and sort them alphabetically and write them in a file\n",
    "cities = dataset['city_or_county'].unique()\n",
    "cities.sort()\n",
    "with open('debugging/cities2.txt', 'w') as f:\n",
    "    for item in cities:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many cities which differ in having a space in the end, let's remove all spaces to avoid problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10416 unique cities in the dataset\n"
     ]
    }
   ],
   "source": [
    "# Remove all spaces from city names\n",
    "dataset['city_or_county'] = dataset['city_or_county'].apply(lambda x: x.replace(\" \", \"\"))\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10331 unique cities in the dataset\n"
     ]
    }
   ],
   "source": [
    "dataset['city_or_county'] = dataset['city_or_county'].str.upper()\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing a week parameter and filtering only cities with a number of weeks with incidents greater than 15% of the total number of the weeks of the 4 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['week'] = \" \"\n",
    "\n",
    "# Date attribute is a progressive integer number, starting from 0\n",
    "# Assign a week number to each date\n",
    "dataset['date'] = dataset['date'] - dataset['date'].min()\n",
    "dataset['week'] = dataset['date'].apply(lambda x: int(x / 7))\n",
    "\n",
    "n_weeks = dataset['week'].max()\n",
    "n_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = dataset['city_or_county'].unique()\n",
    "dropping_threshold = 0.15\n",
    "\n",
    "for city in cities:\n",
    "    city_data = dataset[dataset['city_or_county'] == city]\n",
    "    city_weeks_with_incidents = city_data['week'].nunique()\n",
    "\n",
    "    # Drop the city if it has less than 15% of the weeks with incidents\n",
    "    if city_weeks_with_incidents < n_weeks * dropping_threshold:\n",
    "        dataset = dataset[dataset['city_or_county'] != city]\n",
    "\n",
    "print('Number of cities for which time series will be generated:', dataset['city_or_county'].nunique())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset in csv\n",
    "dataset.to_csv('debugging/ts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Generation with different score functions for each subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import (TimeSeriesScalerMeanVariance,\n",
    "                                   TimeSeriesScalerMinMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions to compute the score for each of the two subtasks. The score used is the <b><u> perceived risk </b></u> computed in an \"eligibility trace fashion\", borrowing this concept from reinforcement learning rewards. We add to the score the number of incidents in the current week, and we sum the score of previous weeks, decaying it by multiplying it by a factor alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold for the number of killed people in a city \n",
    "# to be considered as a label 1 class\n",
    "ISKILLED_THRESHOLD = 25\n",
    "\n",
    "def compute_week_score(week_data, task, prev_score, alpha = 0.5):\n",
    "    \"\"\"Compute the score for a given week, to be used in the time seriesw\n",
    "\n",
    "    Args:\n",
    "        week_data (pandas.DataFrame): Data for a given week\n",
    "        task (str): Task to be performed, used for deciding score function.\n",
    "        prev_score (float): Score of the previous week\n",
    "        alpha (float, optional): Weight for the previous score. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        float: Score for the given week\n",
    "    \"\"\"\n",
    "    if task == 'clustering' or task == 'shapelet_learning' or task == None:\n",
    "        score = prev_score * alpha + week_data.shape[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_time_series_with_label(city_data, n_weeks, task):\n",
    "    \"\"\"Generate the time series for a given city.\n",
    "    Label is referred to the whole time series.\n",
    "\n",
    "    Args:\n",
    "        city_data (pandas.DataFrame): Data for a given city\n",
    "        n_weeks (int): Number of weeks in the dataset\n",
    "        task (str): Task to be performed, used for deciding score function.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Time series for the given city\n",
    "            Shape (n_weeks, )\n",
    "        int: Label for the time series\n",
    "    \"\"\"\n",
    "    # Generate the time series for a given city\n",
    "    time_series = np.zeros(n_weeks)\n",
    "    for week in range(n_weeks):\n",
    "        week_data = city_data[city_data['week'] == week]\n",
    "        if week_data.shape[0] > 0:\n",
    "            time_series[week] = compute_week_score(week_data, task, time_series[week - 1])\n",
    "    \n",
    "    if (city_data['killed_ratio'] * city_data['n_participants']).sum() > ISKILLED_THRESHOLD:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "\n",
    "    return time_series, label\n",
    "\n",
    "def generate_time_series_dataset(dataset, task = None):\n",
    "    \"\"\"Generate the time series dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (pandas.DataFrame): Dataset\n",
    "        task (str, optional): Task to be performed, used for deciding score function. Defaults to None.\n",
    "\n",
    "    Returns:    \n",
    "        list: [Time series dataset, labels]\n",
    "        numpy.ndarray: Cities of the time series dataset\n",
    "    \"\"\"\n",
    "    # Generate the time series for all cities\n",
    "    n_weeks = dataset['week'].max()\n",
    "    cities = dataset['city_or_county'].unique()\n",
    "    time_series = []\n",
    "    labels = []\n",
    "    for city in cities:\n",
    "        city_data = dataset[dataset['city_or_county'] == city]\n",
    "        ts, label = generate_time_series_with_label(city_data, n_weeks, task)\n",
    "        time_series.append(ts)\n",
    "        labels.append(label)\n",
    "        \n",
    "    time_series = np.array(time_series)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    time_series = [time_series, labels]\n",
    "\n",
    "    return time_series, cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSScaler:\n",
    "    \"\"\"Base class to scale time series data\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose = True):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit_transform(self, time_series):\n",
    "        \"\"\"Fit and transform the time series\n",
    "\n",
    "        Args:\n",
    "            time_series (numpy.ndarray): Time series to be scaled\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Scaled time series\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "\n",
    "    def transform(self, time_series):\n",
    "        \"\"\"Transform the time series\n",
    "\n",
    "        Args:\n",
    "            time_series (numpy.ndarray): Time series to be scaled\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Scaled time series\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def inverse_transform(self, time_series):\n",
    "        \"\"\"Inverse transform the time series\n",
    "\n",
    "        Args:\n",
    "            time_series (numpy.ndarray): Time series to be scaled\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Scaled time series\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "    \n",
    "\n",
    "class MinMaxScaler(TSScaler):\n",
    "    \"\"\"Class to scale time series data using minmax scaling\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose = True):\n",
    "        super().__init__(verbose)\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "\n",
    "    def fit_transform(self, time_series):\n",
    "        \"\"\"Fit and transform the time series\n",
    "\n",
    "        Args:\n",
    "            time_series (numpy.ndarray): Time series to be scaled\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Scaled time series\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "        \"\"\"\n",
    "        self.min = time_series.min(axis = 1).min(axis = 0)\n",
    "        self.max = time_series.max(axis = 1).max(axis = 0)\n",
    "        return self.transform(time_series)\n",
    "\n",
    "    def transform(self, time_series):\n",
    "        \"\"\"Transform the time series\n",
    "\n",
    "        Args:\n",
    "            time_series (numpy.ndarray): Time series to be scaled\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Scaled time series\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "        \"\"\"\n",
    "        if self.min is None or self.max is None:\n",
    "            raise ValueError('MinMaxScaler is not fitted')\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Scaling time series with MinMaxScaler')\n",
    "        return (time_series - self.min) / (self.max - self.min)\n",
    "\n",
    "    def inverse_transform(self, time_series):\n",
    "        \"\"\"Inverse transform the time series\n",
    "\n",
    "        Args:\n",
    "            time_series (numpy.ndarray): Time series to be scaled\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Scaled time series\n",
    "                Shape (n_samples, n_timesteps, n_features)\n",
    "        \"\"\"\n",
    "        if self.min is None or self.max is None:\n",
    "            raise ValueError('MinMaxScaler is not fitted')\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Inverse scaling time series with MinMaxScaler')\n",
    "        return time_series * (self.max - self.min) + self.min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, no resampling is necessary as all sequences are of same length, as we define them over 208 weeks. Furthermore, no approximation is needed as the resulting dataset of time series is small.\n",
    "\n",
    "<b>Important!!!</b> Keep in mind that minmax scaler in tslearn scales each time series independently, so the two following time series of killed people in our dataset will look identical:\n",
    "\n",
    "1. 0 - 0 - 100 - 0 - ... - 0\n",
    "2. 0 - 0 -  1  - 0 - ... - 0\n",
    "\n",
    "This is because usually one would like to do amplitude scaling on the dataset (e.g. if the signal is electrical tension, and the important thing is the shape of the signal and not the amplitude itself). However, in our case it is important to consider this loss in information. For this reason, we implement our own minmax scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Generate time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset, cities = generate_time_series_dataset(dataset)\n",
    "X = ts_dataset[0]\n",
    "y = ts_dataset[1]\n",
    "print('Shape of X:', X.shape)   \n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_zscore = TimeSeriesScalerMeanVariance()  \n",
    "scaler_minmax_independent = TimeSeriesScalerMinMax()\n",
    "\n",
    "X_minmax = scaler_minmax.fit_transform(X)\n",
    "X_zscore = scaler_zscore.fit_transform(X)\n",
    "X_minmax_independent = scaler_minmax_independent.fit_transform(X)\n",
    "\n",
    "print('Minmax scaled dataset shape:', X_minmax.shape)\n",
    "print('Zscore scaled dataset shape:', X_zscore.shape)\n",
    "print('Minmax independent scaled dataset shape:', X_minmax_independent.shape)\n",
    "print()\n",
    "print('Shapes of minmax scaled dataset modified for compatibility with tslearn:')\n",
    "X_minmax = np.expand_dims(X_minmax, axis=2)\n",
    "print('Minmax scaled dataset shape:', X_minmax.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the minmax scaled dataset as we defined it with our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = scaler_minmax\n",
    "X = X_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[i])\n",
    "    plt.ylim(0, 1)\n",
    "    if i < 2:\n",
    "        plt.xticks([])\n",
    "    plt.title(cities[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Shape-based Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import KShape, TimeSeriesKMeans\n",
    "from tslearn.clustering import silhouette_score as ts_silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 KMeans with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 20\n",
    "\n",
    "sse_list_km = []\n",
    "silhouette_list_km = []\n",
    "\n",
    "for k in range(2, max_k + 1):\n",
    "    \n",
    "    time_init = time.time()\n",
    "\n",
    "    km = TimeSeriesKMeans(n_clusters=k, metric=\"euclidean\", max_iter=100, random_state=0, n_init=10)\n",
    "    km.fit(X)\n",
    "\n",
    "    if km._iter == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km.append(km.inertia_)\n",
    "    silhouette_list_km.append(ts_silhouette_score(X, km.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km) + 2), sse_list_km)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km) + 2),silhouette_list_km)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km) + 2))\n",
    "\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km) + 2), sse_list_km)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km) + 2), silhouette_list_km)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"euclidean\", max_iter=1000, random_state=0, n_init=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_centers = scaler.inverse_transform(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km_centers.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.title(\"Cluster Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 KMeans with DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTW metric requires long computations, as it has complexity asymptotically different from Euclidean distance. Let's explore how much the cost rises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, n_init=10)\n",
    "km_dtw.fit(X)\n",
    "checkpoint = time.time()\n",
    "print(\"Time elapsed for DTW clustering: %.3f s\" % (time.time() - time_init))\n",
    "silhouette_km_dtw = ts_silhouette_score(X, km_dtw.labels_, metric=\"dtw\")\n",
    "print(\"Time elapsed for DTW silhouette score: %.3f s\" % (time.time() - checkpoint))\n",
    "print(\"Number of iterations during training: %d\" % km_dtw.n_iter_)\n",
    "print(\"Total time elapsed for DTW clustering and silhouette score: %.3f s\" % (time.time() - time_init))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is much slower than the Euclidean metric version. This is to be expected, as computing Euclidean distance is linear complexity in time, whilst DTW isn't.\n",
    " \n",
    "<b>Let's try with some constraints on DTW!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "km_dtw.fit(X)\n",
    "checkpoint = time.time()\n",
    "print(\"Time elapsed for constrained DTW clustering: %.3f s\" % (time.time() - time_init))\n",
    "silhouette_km_dtw = ts_silhouette_score(X, km_dtw.labels_, metric=\"dtw\")\n",
    "print(\"Time elapsed for DTW silhouette score: %.3f s\" % (time.time() - checkpoint))\n",
    "print(\"Number of iterations during training: %d\" % km_dtw.n_iter_)\n",
    "print(\"Total time elapsed for DTW clustering and silhouette score: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that even though constraining dtw helps a lot, aanother expensive part is calculating the silhouette score with dtw. So let's use euclidean distance as a metric in silhouette score even when using dtw for clustering. It is still a valid choice for evaluating the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "km_dtw.fit(X)\n",
    "checkpoint = time.time()\n",
    "print(\"Time elapsed for constrained DTW clustering: %.3f s\" % (time.time() - time_init))\n",
    "silhouette_km_dtw = ts_silhouette_score(X, km_dtw.labels_, metric=\"euclidean\")\n",
    "print(\"Time elapsed for Euclidean silhouette score: %.3f s\" % (time.time() - checkpoint))\n",
    "print(\"Number of iterations during training: %d\" % km_dtw.n_iter_)\n",
    "print(\"Total time elapsed for DTW clustering and silhouette score: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have brought down the time complexity, let's actually run the search for k. Keep max_k = 10 to avoid computing for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 10 \n",
    "\n",
    "sse_list_km_dtw = []\n",
    "silhouette_list_km_dtw = []\n",
    "\n",
    "for k in range(2, max_k + 1):   \n",
    "    time_init = time.time()\n",
    "\n",
    "    km_dtw = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "    km_dtw.fit(X)\n",
    "\n",
    "    if km_dtw._iter == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km_dtw.append(km_dtw.inertia_)        \n",
    "    silhouette_list_km_dtw.append(ts_silhouette_score(X, km_dtw.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_dtw) + 2), sse_list_km_dtw)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_dtw) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_dtw) + 2), silhouette_list_km_dtw)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_dtw) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_dtw) + 2), sse_list_km_dtw)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_dtw) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_dtw) + 2), silhouette_list_km_dtw)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_dtw) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "km_dtw.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_dtw_centers = scaler.inverse_transform(km_dtw.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km_dtw_centers.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.title(\"Cluster Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    print('Number of cities in cluster {}: {}'.format(i, (km_dtw.labels_ == i).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 KShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 10\n",
    "\n",
    "sse_list_ks = []\n",
    "silhouette_list_ks = []\n",
    "\n",
    "for k in range(2, max_k + 1):\n",
    "    \n",
    "    time_init = time.time()\n",
    "    ks = KShape(n_clusters=k, max_iter=100, random_state=2, n_init=10)\n",
    "    ks.fit(X)\n",
    "\n",
    "    if ks._iter == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_ks.append(ks.inertia_)\n",
    "    silhouette_list_ks.append(ts_silhouette_score(X, ks.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_ks) + 2), sse_list_ks)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_ks) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_ks) + 2), silhouette_list_ks)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_ks) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_ks) + 2), sse_list_ks)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_ks) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_ks) + 2), silhouette_list_ks)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_ks) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K   \n",
    "ks = KShape(n_clusters=n_clusters, max_iter=1000, random_state=0, n_init=10)\n",
    "ks.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_centers = scaler.inverse_transform(ks.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks_centers.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.title(\"Cluster Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inertia of the three algorithms\n",
    "print(\"Inertia of the three algorithms:\")\n",
    "print(\"Euclidean k-means:\", km.inertia_)\n",
    "print(\"DTW k-means:\", km_dtw.inertia_)\n",
    "print(\"k-Shape:\", ks.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature-based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import Isomap, TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(values):\n",
    "    features = {\n",
    "        'avg': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'var': np.var(values),\n",
    "        'med': np.median(values),\n",
    "        '10p': np.percentile(values, 10),\n",
    "        '25p': np.percentile(values, 25),\n",
    "        '50p': np.percentile(values, 50),\n",
    "        '75p': np.percentile(values, 75),\n",
    "        '90p': np.percentile(values, 90),\n",
    "        'iqr': np.percentile(values, 75) - np.percentile(values, 25),\n",
    "        'cov': 1.0 * np.mean(values) / np.std(values),\n",
    "        'skw': stats.skew(values),\n",
    "        'kur': stats.kurtosis(values)\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X, (X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [list(calculate_features(x).values())[:-2] for x in X]\n",
    "F = np.array(F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat code for kmeans, only using features as metric\n",
    "sse_list_km_features = []\n",
    "silhouette_list_km_features = []\n",
    "\n",
    "for k in range(2, max_k + 1):   \n",
    "    time_init = time.time()\n",
    "\n",
    "    km_features = KMeans(n_clusters=k, max_iter=100, random_state=0, n_init=10)\n",
    "    km_features.fit(F)\n",
    "\n",
    "    if km_features.n_iter_ == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km_features.append(km_features.inertia_)        \n",
    "    silhouette_list_km_features.append(silhouette_score(X, km_features.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_features) + 2), sse_list_km_features)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_features) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_features) + 2), silhouette_list_km_features)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_features) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_features) + 2), sse_list_km_features)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_features) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_features) + 2), silhouette_list_km_features)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_features) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km_features = KMeans(n_clusters=n_clusters, max_iter=1000, random_state=0, n_init=10)\n",
    "km_features.fit(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an isomap representation of the datapoints and cluster centers\n",
    "\n",
    "F_sparse = sp.csr_matrix(F)\n",
    "F_lil = F_sparse.tolil()\n",
    "\n",
    "iso = Isomap(n_components=2, n_neighbors=5, n_jobs=-1) \n",
    "iso.fit(F_lil)\n",
    "X_iso = iso.transform(F_lil)\n",
    "centers_iso = iso.transform(km_features.cluster_centers_)   \n",
    "\n",
    "# Concatenate features and cluster centers, as tsne is transductive: there is no transform function to apply to cluster centers.\n",
    "F_tsne = np.concatenate((F, km_features.cluster_centers_), axis=0)\n",
    "tsne = TSNE(n_components=2, n_jobs=-1, init='random', perplexity=20)\n",
    "X_tsne = tsne.fit_transform(F_tsne)\n",
    "centers_tsne = X_tsne[-n_clusters:]\n",
    "X_tsne = X_tsne[:-n_clusters]\n",
    "\n",
    "\n",
    "# Do a doubple plot where one is based on clustering and the other is based on the labels\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "# Isomap - Clustering\n",
    "scatter_iso_clustering = ax[0, 0].scatter(X_iso[:, 0], X_iso[:, 1], c=km_features.labels_, cmap='rainbow', label='Cluster')\n",
    "ax[0, 0].scatter(centers_iso[:, 0], centers_iso[:, 1], c='black', s=200, alpha=0.5, label='Centers')\n",
    "ax[0, 0].set_title('Clustering (Isomap)')\n",
    "ax[0, 0].set_xticks([])\n",
    "ax[0, 0].set_yticks([])\n",
    "\n",
    "# Isomap - Labels\n",
    "scatter_iso_labels = ax[0, 1].scatter(X_iso[:, 0], X_iso[:, 1], c=y, cmap='rainbow', label='Label')\n",
    "ax[0, 1].set_title('Labels (Isomap)')\n",
    "ax[0, 1].set_xticks([])\n",
    "ax[0, 1].set_yticks([])\n",
    "\n",
    "# t-SNE - Clustering\n",
    "scatter_tsne_clustering = ax[1, 0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=km_features.labels_, cmap='rainbow', label='Cluster')\n",
    "ax[1, 0].scatter(centers_tsne[:, 0], centers_tsne[:, 1], c='black', s=200, alpha=0.5, label='Centers')\n",
    "ax[1, 0].set_title('Clustering (t-SNE)')\n",
    "ax[1, 0].set_xticks([])\n",
    "ax[1, 0].set_yticks([])\n",
    "\n",
    "# t-SNE - Labels\n",
    "scatter_tsne_labels = ax[1, 1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='rainbow', label='Label')\n",
    "ax[1, 1].set_title('Labels (t-SNE)')\n",
    "ax[1, 1].set_xticks([])\n",
    "ax[1, 1].set_yticks([])\n",
    "\n",
    "# Add Legends\n",
    "legend_iso_clustering = ax[0, 0].legend(handles=[scatter_iso_clustering], loc='upper right')\n",
    "legend_iso_labels = ax[0, 1].legend(handles=[scatter_iso_labels], loc='upper right')\n",
    "legend_tsne_clustering = ax[1, 0].legend(handles=[scatter_tsne_clustering], loc='upper right')\n",
    "legend_tsne_labels = ax[1, 1].legend(handles=[scatter_tsne_labels], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inertia of k-means with extracted features:\", km_features.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Compression-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdm_dist(x, y):\n",
    "    x_str = (' '.join([str(v) for v in x.ravel()])).encode('utf-8')\n",
    "    y_str = (' '.join([str(v) for v in y.ravel()])).encode('utf-8')\n",
    "    return len(zlib.compress(x_str + y_str)) / (len(zlib.compress(x_str)) + len(zlib.compress(y_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = pairwise_distances(X.reshape(X.shape[0], X.shape[1]), metric=cdm_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some clustering on this compression matrix M\n",
    "max_k = 20\n",
    "\n",
    "sse_list_km_comp = []\n",
    "silhouette_list_km_comp = []\n",
    "\n",
    "for k in range(2, max_k + 1):\n",
    "    \n",
    "    time_init = time.time()\n",
    "\n",
    "    km_comp = KMeans(n_clusters=k, max_iter=100, random_state=0, n_init=10)\n",
    "    km_comp.fit(M)\n",
    "\n",
    "    if km_comp.n_iter_ == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km_comp.append(km_comp.inertia_)\n",
    "    silhouette_list_km_comp.append(silhouette_score(X, km_comp.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_comp) + 2), sse_list_km_comp)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_comp) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_comp) + 2), silhouette_list_km_comp)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_comp) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_comp) + 2), sse_list_km_comp)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_comp) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_comp) + 2), silhouette_list_km_comp)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_comp) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km_comp = KMeans(n_clusters=n_clusters, max_iter=1000, random_state=0, n_init=10)\n",
    "km_comp.fit(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an isomap representation of the datapoints and cluster centers\n",
    "\n",
    "M_sparse = sp.csr_matrix(M)\n",
    "M_lil = M_sparse.tolil()\n",
    "\n",
    "iso = Isomap(n_components=2, n_neighbors=5, n_jobs=-1) \n",
    "iso.fit(M_lil)\n",
    "X_iso = iso.transform(M_lil)\n",
    "centers_iso = iso.transform(km_comp.cluster_centers_)   \n",
    "\n",
    "# Concatenate comp and cluster centers, as tsne is transductive: there is no transform function to apply to cluster centers.\n",
    "M_tsne = np.concatenate((M, km_comp.cluster_centers_), axis=0)\n",
    "tsne = TSNE(n_components=2, n_jobs=-1, init='random', perplexity=20)\n",
    "X_tsne = tsne.fit_transform(M_tsne)\n",
    "centers_tsne = X_tsne[-n_clusters:]\n",
    "X_tsne = X_tsne[:-n_clusters]\n",
    "\n",
    "\n",
    "# Do a doubple plot where one is based on clustering and the other is based on the labels\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "# Isomap - Clustering\n",
    "scatter_iso_clustering = ax[0, 0].scatter(X_iso[:, 0], X_iso[:, 1], c=km_comp.labels_, cmap='rainbow', label='Cluster')\n",
    "ax[0, 0].scatter(centers_iso[:, 0], centers_iso[:, 1], c='black', s=200, alpha=0.5, label='Centers')\n",
    "ax[0, 0].set_title('Clustering (Isomap)')\n",
    "ax[0, 0].set_xticks([])\n",
    "ax[0, 0].set_yticks([])\n",
    "\n",
    "# Isomap - Labels\n",
    "scatter_iso_labels = ax[0, 1].scatter(X_iso[:, 0], X_iso[:, 1], c=y, cmap='rainbow', label='Label')\n",
    "ax[0, 1].set_title('Labels (Isomap)')\n",
    "ax[0, 1].set_xticks([])\n",
    "ax[0, 1].set_yticks([])\n",
    "\n",
    "# t-SNE - Clustering\n",
    "scatter_tsne_clustering = ax[1, 0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=km_comp.labels_, cmap='rainbow', label='Cluster')\n",
    "ax[1, 0].scatter(centers_tsne[:, 0], centers_tsne[:, 1], c='black', s=200, alpha=0.5, label='Centers')\n",
    "ax[1, 0].set_title('Clustering (t-SNE)')\n",
    "ax[1, 0].set_xticks([])\n",
    "ax[1, 0].set_yticks([])\n",
    "\n",
    "# t-SNE - Labels\n",
    "scatter_tsne_labels = ax[1, 1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='rainbow', label='Label')\n",
    "ax[1, 1].set_title('Labels (t-SNE)')\n",
    "ax[1, 1].set_xticks([])\n",
    "ax[1, 1].set_yticks([])\n",
    "\n",
    "# Add Legends\n",
    "legend_iso_clustering = ax[0, 0].legend(handles=[scatter_iso_clustering], loc='upper right')\n",
    "legend_iso_labels = ax[0, 1].legend(handles=[scatter_iso_labels], loc='upper right')\n",
    "legend_tsne_clustering = ax[1, 0].legend(handles=[scatter_tsne_clustering], loc='upper right')\n",
    "legend_tsne_labels = ax[1, 1].legend(handles=[scatter_tsne_labels], loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Approximation-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.piecewise import PiecewiseAggregateApproximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paa_segments = 20\n",
    "paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "X_paa = paa.inverse_transform(paa.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat code for kmeans\n",
    "\n",
    "sse_list_km_paa = []\n",
    "silhouette_list_km_paa = []\n",
    "\n",
    "for k in range(2, max_k + 1):\n",
    "    \n",
    "    time_init = time.time()\n",
    "\n",
    "    km_paa = TimeSeriesKMeans(n_clusters=k, metric=\"euclidean\", max_iter=100, random_state=0, n_init=10)\n",
    "    km_paa.fit(X_paa)\n",
    "\n",
    "    if km_paa.n_iter_ == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km_paa.append(km_paa.inertia_)\n",
    "    silhouette_list_km_paa.append(ts_silhouette_score(X, km_paa.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_paa) + 2), sse_list_km_paa)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_paa) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_paa) + 2), silhouette_list_km_paa)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_paa) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_paa) + 2), sse_list_km_paa)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_paa) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_paa) + 2), silhouette_list_km_paa)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_paa) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km_paa = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"euclidean\", max_iter=1000, random_state=0, n_init=10)\n",
    "km_paa.fit(X_paa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_paa_centers = scaler.inverse_transform(km_paa.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km_paa_centers.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.title(\"Cluster Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute the centroid from time series in cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.plot(np.mean(X[np.where(km_paa.labels_ == i)[0]], axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(f'Number of points in cluster {i}: {np.where(km_paa.labels_ == i)[0].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Motif Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Stumpy library calculates the matrix profile for a given time series\n",
    "# Select the city with most weeks with incidents in the dataset and calculate its matrix profile\n",
    "non_zero_values = np.count_nonzero(X, axis=1)\n",
    "city_index = np.argmax(non_zero_values)\n",
    "city = cities[city_index]\n",
    "\n",
    "ts = X[city_index]  \n",
    "print('City with most weeks with incidents:', city)\n",
    "print('Number of weeks with incidents:', non_zero_values[city_index])\n",
    "ts = ts.reshape(ts.shape[0])\n",
    "ts.shape\n",
    "\"\"\"\n",
    "\n",
    "city = 'CHICAGO'\n",
    "\n",
    "# Find index of city in the cities array of cities\n",
    "city_index = np.where(cities == city)\n",
    "\n",
    "# Get the time series of the city\n",
    "ts = X[city_index]\n",
    "ts = ts.reshape(ts.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 5\n",
    "mp = stumpy.stump(ts, w)[:, 0]\n",
    "\n",
    "plt.plot(mp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the motifs and their locations\n",
    "motif_dist, motif_idx = stumpy.motifs(ts, mp, max_motifs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)\n",
    "\n",
    "colors = ['r', 'g', 'k', 'b', 'y'][:len(motif_dist)]\n",
    "\n",
    "for motifs, indices, color in zip(motif_dist, motif_idx, colors):\n",
    "    for index in indices:\n",
    "        motif_shape = ts[index:index + w]\n",
    "        if len(motif_shape) == w:\n",
    "            plt.plot(range(index, index + w), motif_shape, color=color, lw=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for motifs, indices, color in zip(motif_dist, motif_idx, colors):\n",
    "    for index in indices:\n",
    "        motif_shape = ts[index:index + w]\n",
    "        if len(motif_shape) == w:\n",
    "            plt.plot(range(index, index + w), motif_shape, color=color, lw=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate anomalies as out of distribution values of the matrix profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores for the matrix profile\n",
    "z_scores = stats.zscore(mp.tolist())\n",
    "\n",
    "# Define a threshold (e.g., 3 standard deviations)\n",
    "threshold = 3\n",
    "\n",
    "# Find anomalies based on the threshold\n",
    "anomalies = np.where(np.abs(z_scores) > threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)\n",
    "for a in anomalies:\n",
    "    a_shape = ts[a:a+w]\n",
    "    plt.plot(range(a, a+w), a_shape, color='r', lw=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Shapelet Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.shapelets import LearningShapelets, grabocka_params_to_shapelet_size_dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Dataset preparation with labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many time series in dataset have at least one killing and how many don't\n",
    "# This is the label for the shapelet learning task\n",
    "print(f'Number of time series with at least {ISKILLED_THRESHOLD} killings:', np.sum(y))\n",
    "print('Number of time series with no killings:', y.shape[0] - np.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Shapelet Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparams\n",
    "n_ts, ts_sz = X_train.shape[:2]\n",
    "print(n_ts, ts_sz)\n",
    "l = 0.1\n",
    "r = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapelet_dict = grabocka_params_to_shapelet_size_dict(n_ts = n_ts, ts_sz = ts_sz, n_classes = 2, l = l, r = r)\n",
    "shapelet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapelets on the small training set and then use those shapelets for doing the shapelet transform on all setes: the full training set, the validation set and the test set. We will then use these resulting shapelet-transformed sets to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = LearningShapelets(n_shapelets_per_size= shapelet_dict, weight_regularizer=0.0001, batch_size=64, max_iter=2500,\n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=0.003), scale=False)\n",
    "\n",
    "# Fit the model\n",
    "trans.fit(X_train, y_train)\n",
    "shapelets_learned = trans.shapelets_\n",
    "shapelets_learned_as_ts = trans.shapelets_as_time_series_\n",
    "\n",
    "X_train_sh = trans.transform(X_train)\n",
    "X_test_sh = trans.transform(X_test)\n",
    "\n",
    "print('Shapelet transform performed, returning array of shape=(n_ts, n_shapelets) of distances to each shapelet')\n",
    "print(X_train_sh.shape)\n",
    "print(X_test_sh.shape)\n",
    "print('Number of shapelets according to the shapelet dictionary:', sum(shapelet_dict.values()))\n",
    "print('Actual number of shapelets learned:', shapelets_learned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trans.predict(X_test)\n",
    "accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model used by shapelet learner:', repr(trans.model_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of shapelets learned as time series:', shapelets_learned_as_ts.shape)\n",
    "print('Shape of shapelets learned:', shapelets_learned.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shapelets that are as many as we calculated in the dictionary\n",
    "n_shapelet_sizes = len(shapelet_dict.keys())\n",
    "max_shapelet_size = max(shapelet_dict.keys())\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, sz in enumerate(shapelet_dict.keys()):\n",
    "    plt.subplot(n_shapelet_sizes, 1, i + 1)\n",
    "    plt.title(\"%d shapelets of size %d\" % (shapelet_dict[sz], sz))\n",
    "    for sh_idx in range(shapelet_dict[sz]):\n",
    "        plt.plot(shapelets_learned_as_ts[sh_idx + sum(list(shapelet_dict.values())[:i])].ravel())\n",
    "        plt.xlim([0, max_shapelet_size])\n",
    "        plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the paper, multiple similar shapelets is not necessarily a bad thing. In ML, generally, having two similar features might cause multicollinearity and bias, but in the case of shapelets, one shapelet might not be able to separate the data, so multiple similar shapelets may be actually useful.\n",
    "\n",
    "Paper: https://www.ismll.uni-hildesheim.de/pub/pdfs/grabocka2014e-kdd.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Classification with Shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 K-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm='ball_tree', metric='minkowski')\n",
    "param_dist = {\"n_neighbors\": [1, 3, 5, 10, 20]}\n",
    "\n",
    "grid_search = GridSearchCV(knn, param_grid=param_dist,\n",
    "                                 n_jobs=-1,\n",
    "                                 scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_sh, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn = grid_search.best_estimator_\n",
    "best_knn_hp = grid_search.best_params_\n",
    "print('Best setting params ', best_knn_hp)  \n",
    "print('Mean and std of this setting ', grid_search.cv_results_['mean_test_score'][0],\n",
    "      grid_search.cv_results_['std_test_score'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "param_dist = {\"max_depth\": [3, 5, 7, 10, None],\n",
    "              \"max_features\": sp_randint(1, X_train_sh.shape[0] + 1),\n",
    "              \"min_samples_split\": sp_randint(20, 51),\n",
    "              \"min_samples_leaf\": sp_randint(20, 51),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"entropy\", \"gini\"]}\n",
    "\n",
    "random_search = RandomizedSearchCV(random_forest, param_distributions=param_dist,\n",
    "                                 n_iter=10,\n",
    "                                 n_jobs=-1,\n",
    "                                scoring='accuracy')\n",
    "\n",
    "random_search.fit(X_train_sh, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = random_search.best_estimator_\n",
    "best_rf_hp = random_search.cv_results_['params'][0]\n",
    "print('Best setting parameters ', best_rf_hp)\n",
    "print('Mean and std of this setting ', random_search.cv_results_['mean_test_score'][0],\n",
    "      random_search.cv_results_['std_test_score'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "y_pred = best_rf.predict(X_test_sh)\n",
    "print('Accuracy score of best random forest model on test set:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 IsKilled Shapelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the task of finding the shapelet corresponding to the IsKilled class. The approach we experiment on is using a random forest, which is an explainable model, to see what features it relies the most to make a decision. In this way, we'll identify the shapelet best representing the class IsKilled as the shapelet which generates the feature that helps the most the classifier with the binary separation of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature names for the plot. We will use the shapelet indices from the shapelet dictionary.\n",
    "feature_names = []\n",
    "for sz in shapelet_dict.keys():\n",
    "    for sh_idx in range(shapelet_dict[sz]):\n",
    "        feature_names.append(f'Shapelet {sh_idx} (size {sz})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "feature_importance = best_rf.feature_importances_\n",
    "\n",
    "# Sort indices based on feature importance\n",
    "sorted_idx = feature_importance.argsort()[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(sorted_idx)), feature_importance[sorted_idx], align=\"center\")\n",
    "plt.xticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx], rotation=90)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature importances:')\n",
    "for i in sorted_idx:\n",
    "    print(f'{feature_names[i]}: {feature_importance[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapelets_learned_as_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the shapelets back to their original shape\n",
    "shapelets_rescaled = scaler.inverse_transform(shapelets_learned_as_ts)\n",
    "shapelets_rescaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the three most important shapelets according to feature importances and name according to feature names\n",
    "n_plotted = 1\n",
    "\n",
    "shapelets_ordered = shapelets_rescaled[sorted_idx]\n",
    "\n",
    "shapelets_names_ordered = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(n_plotted):\n",
    "    plt.subplot(n_plotted, 1, i + 1)\n",
    "    plt.title(f\"Shapelet {shapelets_names_ordered[i]}\")\n",
    "    plt.plot(shapelets_ordered[i].ravel())\n",
    "    plt.xlim([0, max_shapelet_size])\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Vars to Avoid Rerunning All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate error to interrupt notebook and evaluate if one wants to save vars or less\n",
    "raise ValueError('Stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_VARS = True\n",
    "\n",
    "if SAVE_VARS:\n",
    "    my_shelf = shelve.open(vars_file):\n",
    "    for key in dir():\n",
    "        if key[:2] != \"__\":\n",
    "            try:\n",
    "                my_shelf[key] = globals()[key]\n",
    "                print('SAVED: {0}'.format(key))\n",
    "            except:\n",
    "                \n",
    "                print('\\n ERROR shelving: {0}\\n'.format(key))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
