{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/final_dataset.csv\")\n",
    "print(\"Shape of dataset:\", dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Select cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only incidents regarding [2014, 2015, 2016, 2017], as by project assignment instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[(dataset['year'] > 2013) & (dataset['year'] < 2018)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of cities reveals that many cities are present with different names, resulting in incorrect city value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('debugging/cities.txt', 'w') as f:\n",
    "    for item in dataset['city_or_county'].unique():\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# Write city and value counts of each city to a file\n",
    "with open('debugging/city_counts.txt', 'w') as f:\n",
    "    f.write(dataset['city_or_county'].value_counts().to_string())\n",
    "\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate parenthesis with county or extra information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate all data between parenthesis in the city column using re module\n",
    "dataset['city_or_county'] = dataset['city_or_county'].apply(lambda x: re.sub(r\"\\(.*\\)\", \"\", x))\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort cities alphabetically to see if there are still duplicates and how relevant they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all cities and sort them alphabetically and write them in a file\n",
    "cities = dataset['city_or_county'].unique()\n",
    "cities.sort()\n",
    "with open('debugging/cities2.txt', 'w') as f:\n",
    "    for item in cities:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many cities which differ in having a space in the end, let's remove all spaces to avoid problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all spaces from city names\n",
    "dataset['city_or_county'] = dataset['city_or_county'].apply(lambda x: x.replace(\" \", \"\"))\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['city_or_county'] = dataset['city_or_county'].str.upper()\n",
    "print('There are {} unique cities in the dataset'.format(len(dataset['city_or_county'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing a week parameter and filtering only cities with a number of weeks with incidents greater than 15% of the total number of the weeks of the 4 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['week'] = \" \"\n",
    "\n",
    "# Date attribute is a progressive integer number, starting from 0\n",
    "# Assign a week number to each date\n",
    "dataset['date'] = dataset['date'] - dataset['date'].min()\n",
    "dataset['week'] = dataset['date'].apply(lambda x: int(x / 7))\n",
    "\n",
    "n_weeks = dataset['week'].max()\n",
    "n_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = dataset['city_or_county'].unique()\n",
    "dropping_threshold = 0.15\n",
    "\n",
    "for city in cities:\n",
    "    city_data = dataset[dataset['city_or_county'] == city]\n",
    "    city_weeks_with_incidents = city_data['week'].nunique()\n",
    "\n",
    "    # Drop the city if it has less than 15% of the weeks with incidents\n",
    "    if city_weeks_with_incidents < n_weeks * dropping_threshold:\n",
    "        dataset = dataset[dataset['city_or_county'] != city]\n",
    "\n",
    "print('Number of cities for which time series will be generated:', dataset['city_or_county'].nunique())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset in csv\n",
    "dataset.to_csv('debugging/ts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Generation with different score functions for each subtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import (TimeSeriesScalerMeanVariance,\n",
    "                                   TimeSeriesScalerMinMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions to compute the score for each of the two subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold for the number of killed people in a city \n",
    "# to be considered as a label 1 class\n",
    "ISKILLED_THRESHOLD = 10\n",
    "\n",
    "def compute_week_score(week_data, task):\n",
    "    # Compute the score for a given week, to be used in the time series\n",
    "    if task == 'task1':\n",
    "        score = (week_data['killed_ratio'] * week_data['n_participants']).sum()\n",
    "    elif task == 'shapelet_learning':\n",
    "        score = (week_data['killed_ratio'] * week_data['n_participants']).sum()\n",
    "    return score\n",
    "\n",
    "def generate_time_series(city_data, n_weeks, task):\n",
    "    \"\"\"Generate the time series for a given city\n",
    "\n",
    "    Args:\n",
    "        city_data (pandas.DataFrame): Data for a given city\n",
    "        n_weeks (int): Number of weeks in the dataset\n",
    "        task (str): Task to be performed\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Time series for the given city\n",
    "            Shape (n_weeks, ) if task is 'task1'    \n",
    "            Shape (n_weeks, 2) if task is 'shapelet_learning'\n",
    "    \"\"\"\n",
    "    # Generate the time series for a given city\n",
    "    time_series = np.zeros(n_weeks)\n",
    "    for week in range(n_weeks):\n",
    "        week_data = city_data[city_data['week'] == week]\n",
    "        if week_data.shape[0] > 0:\n",
    "            time_series[week] = compute_week_score(week_data, task)\n",
    "\n",
    "    return time_series\n",
    "\n",
    "def generate_time_series_with_label(city_data, n_weeks, task):\n",
    "    \"\"\"Generate the time series for a given city.\n",
    "    Label is referred to the whole time series.\n",
    "\n",
    "    Args:\n",
    "        city_data (pandas.DataFrame): Data for a given city\n",
    "        n_weeks (int): Number of weeks in the dataset\n",
    "        task (str): Task to be performed\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Time series for the given city\n",
    "            Shape (n_weeks, )\n",
    "        int: Label for the time series\n",
    "    \"\"\"\n",
    "    # Generate the time series for a given city\n",
    "    time_series = np.zeros(n_weeks)\n",
    "    for week in range(n_weeks):\n",
    "        week_data = city_data[city_data['week'] == week]\n",
    "        if week_data.shape[0] > 0:\n",
    "            time_series[week] = compute_week_score(week_data, 'shapelet_learning')\n",
    "    \n",
    "    if (city_data['killed_ratio'] * city_data['n_participants']).sum() > ISKILLED_THRESHOLD:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "\n",
    "    return time_series, label\n",
    "\n",
    "def generate_time_series_dataset(dataset, task, scaling = None, verbose = True):\n",
    "    \"\"\"Generate the time series dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (pandas.DataFrame): Dataset\n",
    "        task (str): Task to be performed\n",
    "        scaling (str, optional): Scaling method to be used. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        if task is 'task1':\n",
    "            numpy.ndarray: Time series dataset\n",
    "        elif task is 'shapelet_learning':\n",
    "            list: [Time series dataset, labels]\n",
    "        \n",
    "        numpy.ndarray: Cities of the time series dataset\n",
    "    \"\"\"\n",
    "    # Generate the time series for all cities\n",
    "    n_weeks = dataset['week'].max()\n",
    "    cities = dataset['city_or_county'].unique()\n",
    "    time_series = []\n",
    "    labels = []\n",
    "    for city in cities:\n",
    "        city_data = dataset[dataset['city_or_county'] == city]\n",
    "\n",
    "        if task == 'shapelet_learning':\n",
    "            ts, label = generate_time_series_with_label(city_data, n_weeks, task)\n",
    "            time_series.append(ts)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            time_series.append(generate_time_series(city_data, n_weeks, task))\n",
    "        \n",
    "    time_series = np.array(time_series)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Scale the time series\n",
    "    if scaling == 'minmax_independent':\n",
    "        time_series = TimeSeriesScalerMinMax().fit_transform(time_series)\n",
    "    elif scaling == 'minmax_global':\n",
    "        time_series = minmax_scaling(time_series, verbose)\n",
    "    elif scaling == 'zscore':\n",
    "        time_series = TimeSeriesScalerMeanVariance().fit_transform(time_series)\n",
    "    elif scaling == 'none' or scaling is None:\n",
    "        # Add the dimensionality as the scalers do for consistency\n",
    "        time_series = time_series.reshape(time_series.shape[0], time_series.shape[1], 1)\n",
    "\n",
    "    if task == 'shapelet_learning':\n",
    "        # Add the labels to the time series, adjusting the shape of labels \n",
    "        time_series = [time_series, labels]\n",
    "\n",
    "    return time_series, cities\n",
    "\n",
    "def minmax_scaling(time_series, verbose = True):\n",
    "    \"\"\"Scale the time series using minmax scaling, getting max score of time series\n",
    "    and min score of time series, and scaling each time series with same max and min.\n",
    "\n",
    "    Args:\n",
    "        time_series (numpy.ndarray): Time series dataset of shape (n_samples, n_weeks)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Scaled time series dataset of shape (n_samples, n_weeks)\n",
    "    \"\"\"\n",
    "    max_score = time_series.max()\n",
    "    min_score = time_series.min()\n",
    "    time_series = (time_series - min_score) / (max_score - min_score)\n",
    "    if verbose:\n",
    "        print('Max score:', max_score)\n",
    "        print('Min score:', min_score)\n",
    "    return time_series\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, no resampling is necessary as all sequences are of same length, as we define them over 208 weeks. Furthermore, no approximation is needed as the resulting dataset of time series is small.\n",
    "\n",
    "<b>Important!!!</b> Keep in mind that minmax scaler in tslearn scales each time series independently, so the two following time series of killed people in our dataset will look identical:\n",
    "\n",
    "1. 0 - 0 - 100 - 0 - ... - 0\n",
    "2. 0 - 0 -  1  - 0 - ... - 0\n",
    "\n",
    "This is because usually one would like to do amplitude scaling on the dataset (e.g. if the signal is electrical tension, and the important thing is the shape of the signal and not the amplitude itself). However, in our case it is important to consider this loss in information. For this reason, we implement our own minmax scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Generate time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset, cities = generate_time_series_dataset(dataset, task = 'task1', scaling = 'minmax_global')\n",
    "ts_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ts_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[i])\n",
    "    plt.ylim(0, 1)\n",
    "    if i < 2:\n",
    "        plt.xticks([])\n",
    "    plt.title(cities[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Shape-based Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import KShape, TimeSeriesKMeans\n",
    "from tslearn.clustering import silhouette_score as ts_silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 KMeans with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 20\n",
    "\n",
    "sse_list_km = []\n",
    "silhouette_list_km = []\n",
    "\n",
    "for k in range(2, max_k + 1):\n",
    "    \n",
    "    time_init = time.time()\n",
    "\n",
    "    km = TimeSeriesKMeans(n_clusters=k, metric=\"euclidean\", max_iter=100, random_state=0, n_init=10)\n",
    "    km.fit(X)\n",
    "\n",
    "    if km._iter == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km.append(km.inertia_)\n",
    "    silhouette_list_km.append(ts_silhouette_score(X, km.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km) + 2), sse_list_km)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km) + 2),silhouette_list_km)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km) + 2))\n",
    "\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km) + 2), sse_list_km)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km) + 2), silhouette_list_km)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"euclidean\", max_iter=1000, random_state=0, n_init=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km.cluster_centers_.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.title(\"Cluster Centers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 KMeans with DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTW metric requires long computations, as it has complexity asymptotically different from Euclidean distance. Let's explore how much the cost rises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, n_init=10)\n",
    "km_dtw.fit(X)\n",
    "checkpoint = time.time()\n",
    "print(\"Time elapsed for DTW clustering: %.3f s\" % (time.time() - time_init))\n",
    "silhouette_km_dtw = ts_silhouette_score(X, km_dtw.labels_, metric=\"dtw\")\n",
    "print(\"Time elapsed for DTW silhouette score: %.3f s\" % (time.time() - checkpoint))\n",
    "print(\"Number of iterations during training: %d\" % km_dtw.n_iter_)\n",
    "print(\"Total time elapsed for DTW clustering and silhouette score: %.3f s\" % (time.time() - time_init))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is much slower than the Euclidean metric version. This is to be expected, as computing Euclidean distance is linear complexity in time, whilst DTW isn't.\n",
    " \n",
    "<b>Let's try with some constraints on DTW!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "km_dtw.fit(X)\n",
    "checkpoint = time.time()\n",
    "print(\"Time elapsed for constrained DTW clustering: %.3f s\" % (time.time() - time_init))\n",
    "silhouette_km_dtw = ts_silhouette_score(X, km_dtw.labels_, metric=\"dtw\")\n",
    "print(\"Time elapsed for DTW silhouette score: %.3f s\" % (time.time() - checkpoint))\n",
    "print(\"Number of iterations during training: %d\" % km_dtw.n_iter_)\n",
    "print(\"Total time elapsed for DTW clustering and silhouette score: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that even though constraining dtw helps a lot, actually the most expensive part is calculating the silhouette score with dtw. So let's use euclidean distance as a metric in silhouette score even when using dtw for clustering. It is still a valid choice for evaluating the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "km_dtw.fit(X)\n",
    "checkpoint = time.time()\n",
    "print(\"Time elapsed for constrained DTW clustering: %.3f s\" % (time.time() - time_init))\n",
    "silhouette_km_dtw = ts_silhouette_score(X, km_dtw.labels_, metric=\"euclidean\")\n",
    "print(\"Time elapsed for Euclidean silhouette score: %.3f s\" % (time.time() - checkpoint))\n",
    "print(\"Number of iterations during training: %d\" % km_dtw.n_iter_)\n",
    "print(\"Total time elapsed for DTW clustering and silhouette score: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have brought down the time complexity, let's actually run the search for k. Keep max_k = 10 to avoid computing for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 10 \n",
    "\n",
    "sse_list_km_dtw = []\n",
    "silhouette_list_km_dtw = []\n",
    "\n",
    "for k in range(2, max_k + 1):   \n",
    "    time_init = time.time()\n",
    "\n",
    "    km_dtw = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "    km_dtw.fit(X)\n",
    "\n",
    "    if km_dtw._iter == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km_dtw.append(km_dtw.inertia_)        \n",
    "    silhouette_list_km_dtw.append(ts_silhouette_score(X, km_dtw.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_dtw) + 2), sse_list_km_dtw)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_dtw) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_dtw) + 2), silhouette_list_km_dtw)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_dtw) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_dtw) + 2), sse_list_km_dtw)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_dtw) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_dtw) + 2), silhouette_list_km_dtw)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_dtw) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", max_iter=100, random_state=0, \\\n",
    "                          metric_params={\"global_constraint\": \"sakoe_chiba\", \"sakoe_chiba_radius\": 3}, n_init=10)\n",
    "km_dtw.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km_dtw.cluster_centers_.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 KShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_k = 20\n",
    "\n",
    "sse_list_ks = []\n",
    "silhouette_list_ks = []\n",
    "\n",
    "for k in range(2, max_k + 1):\n",
    "    \n",
    "    time_init = time.time()\n",
    "    ks = KShape(n_clusters=k, max_iter=100, random_state=0, n_init=10)\n",
    "    ks.fit(X)\n",
    "\n",
    "    if ks._iter == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_ks.append(ks.inertia_)\n",
    "    silhouette_list_ks.append(ts_silhouette_score(X, ks.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_ks) + 2), sse_list_ks)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_ks) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_ks) + 2), silhouette_list_ks)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_ks) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_ks) + 2), sse_list_ks)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_ks) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_ks) + 2), silhouette_list_ks)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_ks) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K   \n",
    "ks = KShape(n_clusters=n_clusters, max_iter=1000, random_state=0)\n",
    "ks.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks.cluster_centers_.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inertia of the three algorithms\n",
    "print(\"Inertia of the three algorithms:\")\n",
    "print(\"Euclidean k-means:\", km.inertia_)\n",
    "print(\"DTW k-means:\", km_dtw.inertia_)\n",
    "print(\"k-Shape:\", ks.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature-based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(values):\n",
    "    features = {\n",
    "        'avg': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'var': np.var(values),\n",
    "        'med': np.median(values),\n",
    "        '10p': np.percentile(values, 10),\n",
    "        '25p': np.percentile(values, 25),\n",
    "        '50p': np.percentile(values, 50),\n",
    "        '75p': np.percentile(values, 75),\n",
    "        '90p': np.percentile(values, 90),\n",
    "        'iqr': np.percentile(values, 75) - np.percentile(values, 25),\n",
    "        'cov': 1.0 * np.mean(values) / np.std(values),\n",
    "        'skw': stats.skew(values),\n",
    "        'kur': stats.kurtosis(values)\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [list(calculate_features(x).values())[:-2] for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat code for kmeans, only using features as metric\n",
    "sse_list_km_features = []\n",
    "silhouette_list_km_features = []\n",
    "\n",
    "for k in range(2, max_k + 1):   \n",
    "    time_init = time.time()\n",
    "\n",
    "    km_features = KMeans(n_clusters=k, max_iter=100, random_state=0, n_init=10)\n",
    "    km_features.fit(X)\n",
    "\n",
    "    if km_features._iter == 100:\n",
    "        print(\"Warning: training did not converge for k = %d.\" % k)\n",
    "\n",
    "    sse_list_km_features.append(km_features.inertia_)        \n",
    "    silhouette_list_km_features.append(ts_silhouette_score(X, km_features.labels_, metric=\"euclidean\"))\n",
    "\n",
    "    print(\"Finished clustering for k = %d;\" % k, \"time elapsed: %.3f s\" % (time.time() - time_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_features) + 2), sse_list_km_features)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_features) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_features) + 2), silhouette_list_km_features)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_features) + 2))\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose K based on the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOSEN_K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax[0].plot(range(2, len(sse_list_km_features) + 2), sse_list_km_features)\n",
    "ax[0].set_ylabel(\"SSE\", fontsize=22)\n",
    "ax[0].set_xticks(range(2, len(sse_list_km_features) + 2))\n",
    "\n",
    "ax[1].plot(range(2, len(silhouette_list_km_features) + 2), silhouette_list_km_features)\n",
    "ax[1].set_ylabel(\"Silhouette Score\", fontsize=22)\n",
    "ax[1].set_xticks(range(2, len(silhouette_list_km_features) + 2))\n",
    "\n",
    "# set a vertical line on all three subplots at x = CHOSEN_K\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x=CHOSEN_K, color=\"r\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"K\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = CHOSEN_K\n",
    "km_features = KMeans(n_clusters=n_clusters, max_iter=1000, random_state=0, n_init=10)\n",
    "km_features.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km_features.cluster_centers_.reshape(n_clusters, X.shape[1]).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inertia of k-means with extracted features:\", km_features.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Compression-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdm_dist(x, y):\n",
    "    x_str = (' '.join([str(v) for v in x.ravel()])).encode('utf-8')\n",
    "    y_str = (' '.join([str(v) for v in y.ravel()])).encode('utf-8')\n",
    "    return len(zlib.compress(x_str + y_str)) / (len(zlib.compress(x_str)) + len(zlib.compress(y_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = pairwise_distances(X.reshape(X.shape[0], X.shape[1]), metric=cdm_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.965, min_samples=5, metric='precomputed')\n",
    "dbscan.fit(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Approximation-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.piecewise import PiecewiseAggregateApproximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paa_segments = 10\n",
    "paa = PiecewiseAggregateApproximation(n_segments=n_paa_segments)\n",
    "X_paa = paa.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_paa.reshape(X_paa.shape[1], X_paa.shape[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\", max_iter=5, random_state=0)\n",
    "km.fit(X_paa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(km.cluster_centers_.reshape(X_paa.shape[1], 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recompute the centroid from time series in cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.plot(np.mean(X[np.where(km.labels_ == i)[0]], axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Motif Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stumpy library calculates the matrix profile for a given time series\n",
    "# Select the city with most weeks with incidents in the dataset and calculate its matrix profile\n",
    "non_zero_values = np.count_nonzero(X, axis=1)\n",
    "city_index = np.argmax(non_zero_values)\n",
    "city = cities[city_index]\n",
    "\n",
    "ts = X[city_index]  \n",
    "print('City with most weeks with incidents:', city)\n",
    "print('Number of weeks with incidents:', non_zero_values[city_index])\n",
    "ts = ts.reshape(ts.shape[0])\n",
    "ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "\n",
    "plt.plot(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 10\n",
    "mp = stumpy.stump(ts, w)[:, 0]\n",
    "\n",
    "plt.plot(mp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the motifs and their locations\n",
    "motif_dist, motif_idx = stumpy.motifs(ts, mp, max_motifs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)\n",
    "\n",
    "colors = ['r', 'g', 'k', 'b', 'y'][:len(motif_dist)]\n",
    "\n",
    "for motifs, indices, color in zip(motif_dist, motif_idx, colors):\n",
    "    for index in indices:\n",
    "        motif_shape = ts[index:index + w]\n",
    "        if len(motif_shape) == w:\n",
    "            plt.plot(range(index, index + w), motif_shape, color=color, lw=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for motifs, indices, color in zip(motif_dist, motif_idx, colors):\n",
    "    for index in indices:\n",
    "        motif_shape = ts[index:index + w]\n",
    "        if len(motif_shape) == w:\n",
    "            plt.plot(range(index, index + w), motif_shape, color=color, lw=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate anomalies as out of distribution values of the matrix profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores for the matrix profile\n",
    "z_scores = stats.zscore(mp.tolist())\n",
    "\n",
    "# Define a threshold (e.g., 3 standard deviations)\n",
    "threshold = 3\n",
    "\n",
    "# Find anomalies based on the threshold\n",
    "anomalies = np.where(np.abs(z_scores) > threshold)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts)\n",
    "for a in anomalies:\n",
    "    a_shape = ts[a:a+w]\n",
    "    plt.plot(range(a, a+w), a_shape, color='r', lw=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Shapelet Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.shapelets import LearningShapelets, grabocka_params_to_shapelet_size_dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Dataset preparation with labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dataset, cities = generate_time_series_dataset(dataset, task = 'shapelet_learning', scaling = 'minmax')\n",
    "\n",
    "X = ts_dataset[0]\n",
    "y = ts_dataset[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many time series in dataset have at least one killing and how many don't\n",
    "# This is the label for the shapelet learning task\n",
    "print('Number of time series with at least one killing:', np.sum(y))\n",
    "print('Number of time series with no killings:', y.shape[0] - np.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Shapelet Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparams\n",
    "n_ts, ts_sz = X_train.shape[:2]\n",
    "l = 0.05\n",
    "r = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow github repo https://github.com/stephanielees/shapeletsLTS-humansensor/blob/main/shapelet-tslearn.ipynb\n",
    "PERCENTAGE = 0.1\n",
    "# Select randomly 10% of the time series in the training set to extract shapelets from\n",
    "indexes = np.random.choice(X_train.shape[0], int(X_train.shape[0] * PERCENTAGE), replace=False)\n",
    "X_train_small = X_train[indexes]\n",
    "y_train_small = y_train[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapelet_dict = grabocka_params_to_shapelet_size_dict(n_ts = n_ts, ts_sz = ts_sz, n_classes = 2, l = l, r = r)\n",
    "shapelet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapelets on the small training set and then use those shapelets for doing the shapelet transform on all setes: the full training set, the validation set and the test set. We will then use these resulting shapelet-transformed sets to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = LearningShapelets(weight_regularizer=0.001, batch_size=512, max_iter=2500, total_lengths=5,\n",
    "                          optimizer=tf.keras.optimizers.Adam(learning_rate=0.003), scale=False)\n",
    "\n",
    "# Fit the model\n",
    "trans.fit(X_train_small, y_train_small)\n",
    "shapelets_learned = trans.shapelets_\n",
    "shapelets_learned_as_ts = trans.shapelets_as_time_series\n",
    "\n",
    "train_shapelets = trans.transform(X_train)\n",
    "valid_shapelets = trans.transform(X_val)\n",
    "test_shapelets = trans.transform(X_test)\n",
    "\n",
    "print(train_shapelets.shape)\n",
    "print(valid_shapelets.shape)\n",
    "print(test_shapelets.shape)\n",
    "print('Number of shapelets according to the shapelet dictionary:', sum(shapelet_dict.values()))\n",
    "print('Actual number of shapelets learned:', shapelets_learned.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the shapelets that are as many as we calculated in the dictionary\n",
    "for shapelet_len, n_shapelets in shapelet_dict.items():  \n",
    "    plt.figure(figsize=(8,5))\n",
    "    for j in range(n_shapelets):\n",
    "        plt.plot(shapelets_learned_as_ts[shapelet_len][j], label='Shapelet length {}'.format(shapelet_len))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the paper, multiple similar shapelets is not necessarily a bad thing. In ML, generally, having two similar features might cause multicollinearity and bias, but in the case of shapelets, one shapelet might not be able to separate the data, so multiple similar shapelets may be actually useful.\n",
    "\n",
    "Paper: https://www.ismll.uni-hildesheim.de/pub/pdfs/grabocka2014e-kdd.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Classification with Shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm.LGBMClassifier(n_estimators = 3500, random_state=8)\n",
    "model.fit(train_shapelets, y_train, eval_set=[(valid_shapelets, y_val)], eval_metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the model on the test set\n",
    "y_pred = model.predict(test_shapelets)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print('Accuracy on the test set:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
